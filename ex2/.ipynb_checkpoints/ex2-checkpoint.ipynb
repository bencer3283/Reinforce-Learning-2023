{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 2 - Value Iteration </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Sailor Gridworld </a>\n",
    "* <a href='#3.'> 3. Value Iteration </a>\n",
    "* <a href='#4.'> 4. Submitting </a>\n",
    "* <a href='#4.1'> 4.1 Feedback </a>\n",
    "* <a href='#5.'> References</a>\n",
    "\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing Value Iteration (30 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Reinforcement Learning Components (5 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 1.2</b> Value Analysis (5 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 1.3</b> Investigating Optimal Path (5 points) </a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Investigating Convergence Properties (15 points) </a>\\\n",
    "<a href='#T3'><b>Student Task 3.</b> Number of Iterations Until Convergence (5 points) </a>\\\n",
    "<a href='#T4'><b>Student Task 4.</b> Evaluating the Policy (10 points) </a>\\\n",
    "<a href='#Q4'><b>Student Question 4.1</b> Relationship Between Discounted Return and the Value Function (10 points)</a>\\\n",
    "<a href='#Q5'><b>Student Question 4.2</b> Considering Unknown Environments (15 points) </a>\n",
    "\n",
    "    \n",
    "**Total Points:** 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f0b31",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In this exercise we look at how a simple method like value iteration can be used to find an optimal behaviour for a sailing gridworld environment. We further investigate the properties of value iteration and how environment definition can affect the behaviour learned by the agent.\n",
    "\n",
    "\n",
    "## 1.1 Learning Objectives: <a id='1.1'></a>\n",
    "- To understand the value iteration method and how it works\n",
    "- To understand the importance of environment definition on behaviours\n",
    "\n",
    "\n",
    "## 1.2 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "You don‚Äôt have to edit any other file other than ```ex2.ipynb``` to complete this exercise.\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                 # Images used in notebook\n",
    "‚îÇ   ex2.ipynb            # Main assignment file containing tasks\n",
    "‚îÇ   feedback.ipynb       # Please give feedback in here\n",
    "‚îÇ   sailing.py           # Defines the sailing gridworld environment\n",
    "‚îÇ   utils.py             # Contains save object function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df707adf",
   "metadata": {},
   "source": [
    "# 2. Sailor Gridworld <a id='2.'></a>\n",
    "\n",
    "Consider a sailor who managed to escape from a sinking ship, and now has to find the way to the nearest harbour. The sea is divided into a grid, with each grid cell corresponding to a state. Therefore, the state can be thought of as a two dimensional vector:\n",
    "\n",
    "$$\n",
    "s=\\left(\\begin{array}{c}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "There are four actions available: moving left, right, up, and down. When the sailor reaches the harbour, the episode terminates and a reward of 10 is given. If the sailor hits the rocks, the episode terminates and a reward of ‚àí2 is given. On all other steps, the reward is 0. The environment is shown in Figure 1. \n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/sailor1.png\" width=\"400px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 1: The Sailor gridworld environment. Light blue squares represent the calm part of the sea, gray squares ‚Äì the rocks, dark blue ‚Äì the windy passage between the rocks. The green square in upper right corner is the target harbour. The current (in this picture also the initial) position of the sailor is denoted with a brown \"boat\"  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The shortest way to the harbour goes through a narrow passage between rocks, which is known to have unpredictable heavy wind conditions. When moving in that area, the sailor can be carried an extra \"square\" in a random direction ‚Äî that is, land in any of the squares adjacent to the desired target square. This is shown in Figure 2. \n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/sailor2.png\" width=\"700px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2: Possible state transitions in windy passage when the issued action was to go left (a) or to go up (b). The sailor may end up in the square to the left (a) or up (b), as indicated by the green arrow. There is also a small $p_{wind}$ that the sailor will move for an additional unit in a random direction, as indicated by one of the yellow arrows. Therefore, in addition to moving one square in the target direction, it can (1) move two squares in the desired direction, (2) stay in place, or (3) be carried sideways to one of the squares on the diagonal. </figcaption>\n",
    "</figure>\n",
    "\n",
    "The sea around the passage is generally calm, but there is a low probability that the sailor will be carried in the direction perpendicular to where he was heading, as shown in Figure 3. \n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/sailor3.png\" width=\"300px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 3: Possible state transitions in calm water when the issued action was to go right. The sailor may end up in the square to the right, as indicated by the green arrow. There is a small chance $p_{calm}$ that the sailor will move in the perpendicular direction, as indicated by the yellow arrows.  </figcaption>\n",
    "</figure>\n",
    "\n",
    "All of these probabilities ($p_{calm}$ and $p_{wind}$) as well as the effects of the wind and the exact location of the harbour are perfectly known to the sailor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0897c2e6",
   "metadata": {},
   "source": [
    "# 3. Value iteration <a id='3.'></a>\n",
    "\n",
    "Value iteration is a method for computing an optimal MDP (Markov Decision Process) policy. We start with arbitrary initial state values and iteratively update our estimate of every state‚Äôs value by using Bellman equation as an update rule. A more detailed description, along with the exact equations, can be found in [1] Section 4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c855ac0",
   "metadata": {},
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing Value Iteration (30 points) </h3> \n",
    "\n",
    "Implement value iteration for the sailor using the code below as a template, assuming the discount factor value $\\gamma = 0.9$. In addition to the state values, compute the policy ‚Äì path to the harbour, using computed state values. Run your implementation for 100 iterations. Render the values and policy after every iteration and observe how the values and policy are updated. Also, **run the program a few times and check if the sailor is able to reach the goal every time**. Ensure the values and policy are saved as a ```.pkl``` file using the ```u.save_object()``` cell below . This ```.pkl``` file is saved within the same directory as this file. **Attach an image of the estimated state values and policy into your submission.**\n",
    "\n",
    "**Hint:** The environment contains a 3-D array (```env.transitions```) of shape $[n_x, n_y, n_a]$, which contains all possible state transitions. The transitions for state $(x, y)$ with action a can be accessed by ```env.transitions [x, y, a]``` . This will return a list of Python ***named-tuples*** Transition=$(next_state, reward, done, prob)$. The components of the named-tuple can be accessed as ```transition.state```, ```transition.reward```, ```transition.done```, ```transition.prob```. For example, ```env.transitions [3, 3, env.UP]``` would return a list of three possible state transitions:\n",
    "\n",
    "```\n",
    "(state=(3, 2), reward=0.0, done=0.0, prob=0.05),\n",
    "(state=(3, 4), reward=0.0, done=0.0, prob=0.05),\n",
    "(state=(4, 3), reward=0.0, done=0.0, prob=0.9)\n",
    "```\n",
    "\n",
    "which corresponds to moving to state (4, 3) with probability 0.9, or moving to states (3, 4) and (3, 2) with probability 0.05 for each. None of these transitions results in a reward or in terminating the episode (the second and third elements are zero). When the episode has already terminated, the next state will be set to ```None```.\n",
    "\n",
    "**Caveat:** Pay extra attention to indices in the Bellman equation ‚Äì specifically, where $V_k$ and where $V_{k‚àí1}$ must be used.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8420398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "from sailing import SailingGridworld\n",
    "import utils as u\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "env = SailingGridworld(rock_penalty=-2, value_update_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_policy(num_iterations=100, gamma=0.9, eps=1e-4, plot_values=False, filename='values.gif'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_iterations: num of iterations to update the value function and policy\n",
    "        gamma: discount factor used in value iteration\n",
    "        eps: a tiny constant used to check the convergence of the value function and policy\n",
    "        plot_values: if True, a GIF will be saved to plot state value functions and policy\n",
    "        filename: filename of the saved value plot, if plot_value is True.\n",
    "    Returns:\n",
    "        updated v_est and policy, both are np.ndarray with shape (env.w, env.h)\n",
    "    \"\"\"\n",
    "    \n",
    "    # value table V_t, it stores the value at a position (x, y).\n",
    "    v_est = np.zeros((env.w, env.h)) # env.w, env.h: width and height of the environment shown in Fig 1.\n",
    "    # policy table pi_t, it stores the current action to take at a position (x, y)\n",
    "    policy = np.zeros((env.w, env.h))\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        ########## Your code starts here ##########\n",
    "        # TODO: Task 1, implement value iteration and retrive the resulting policy.\n",
    "        #       In each iteration, update the state values v_est and policy with new values/policy.\n",
    "        #       The v_est is updated according to the value iteration formula.\n",
    "        # TODO: Task 2, check the convergence of the value function and policy.\n",
    "        #       In each iteration, check the maximal absolute difference in successive v_est/policy,\n",
    "        #       and compare it with eps, e.g., check the difference between v_est_i and v_est_i-1\n",
    "        \n",
    "        \n",
    "        ########## Your code starts here ##########\n",
    "    \n",
    "    if plot_values:\n",
    "        # Call env.record_values_policy function with your estimated state values and policy\n",
    "        # to produce a GIF of the changing values. In our case, we only record the final state values and policy to save running time.\n",
    "        # Call env.save_values_policy function to save the recorded GIF to filename.gif\n",
    "        env.record_values_policy(v_est, policy)\n",
    "        env.save_values_policy(filename=filename)\n",
    "\n",
    "    return v_est, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value iteration -- update value estimation and policy\n",
    "value_est, policy = get_values_policy(num_iterations=100, gamma=0.9, eps=1e-4, plot_values=True, filename='T1_values.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(policy, N=1, gamma=0.9, render=False, filename='env.gif'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policy: the policy to evaluate.\n",
    "        N: number of evaluation episode. \n",
    "            e.g., N=10 means the policy is evaluated 10 times.\n",
    "        gamma: discount factor to calculate the discounted_return.\n",
    "        render: if True, a GIF of the policy's behavior will be saved.\n",
    "        filename: filename to save the GIF file.\n",
    "    Returns:\n",
    "        mean and stddev of discounted_returns.\n",
    "    \"\"\"\n",
    "    discounted_returns = np.empty((N,))\n",
    "    # Eval policy\n",
    "    for ep in range(N):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        discounted_return = 0\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            ########## You code starts here ##########\n",
    "\n",
    "            # TODO: Use the policy to take the optimal action (Task 1)\n",
    "            action = int(np.random.random() * 4)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # TODO: Accumulate discounted return for this episode\n",
    "            discounted_return += 0\n",
    "            steps += 1\n",
    "            \n",
    "            ########## You code ends here ##########\n",
    "\n",
    "            if render:\n",
    "                env.render(filename=filename)\n",
    "        # Record the discounted return for         \n",
    "        discounted_returns[ep] = discounted_return\n",
    "    \n",
    "    mean_discounted_returns = np.mean(discounted_returns)\n",
    "    std_discounted_returns = np.std(discounted_returns)\n",
    "    \n",
    "    return mean_discounted_returns, std_discounted_returns\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b63b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_discounted_returns, std_discounted_returns = eval_policy(policy=policy, N=1, gamma=0.9, render=True, filename='T1_eval.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b136626",
   "metadata": {},
   "source": [
    "To see how the sailboat traverses the gridworld use this cell below to display the created gif:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703fccb-2f3e-454f-a4ef-980eca3d6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(data=open('T1_eval.gif','rb').read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706400e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(data=open('T1_values.gif','rb').read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save value and policy\n",
    "u.save_object({'value': value_est, 'policy': policy}, './value_policy.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32ef45",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Reinforcement Learning Components (5 points) </h3> \n",
    "\n",
    "What is the agent and the environment in this sailor gridworld?\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4a03a",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a8080",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> Value Analysis (5 points) </h3> \n",
    "\n",
    "What is the state value of the harbour and rock states? Why?\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262407d",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f9f21",
   "metadata": {},
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.3</b> Investigating Optimal Path (5 points) </h3> \n",
    "\n",
    "Which path did the sailor choose, the safe path below the rocks, or the dangerous path between the rocks? If you change the reward for hitting the rocks to -10 (that is, make the sailor value life more), does he still choose the same path?\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SailingGridworld(rock_penalty=-10, value_update_iter=100) # Set up env\n",
    "value_est, policy = get_values_policy(num_iterations=100, gamma=0.9, eps=1e-4, plot_values=False) # Run value iteration\n",
    "mean_discounted_returns, std_discounted_returns = eval_policy(policy=policy, N=1, gamma=0.9, render=True, filename='Q1_3.gif') # Evaluate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(data=open('Q1_3.gif','rb').read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d440003",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecca922",
   "metadata": {},
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Investigating Convergence Properties (15 points) </h3> \n",
    "\n",
    "What happens if you run the algorithm for 30 iterations? Do the value function and policy still converge? For the value function, you can assume they have converged\n",
    "if the maximum change in value is lower than a certain threshold $\\epsilon = 10^{-4}$:\n",
    "$$\n",
    "\\max _s\\left|V_k(s)-V_{k-1}(s)\\right|<\\epsilon\n",
    "$$\n",
    "\n",
    "where $V_k(s)$ is the estimated value of state $s$ in $k$-th iteration of the algorithm.\n",
    "Generally, **which of them** - the policy or value function - needs less iterations to converge, if any? **Justify your answer**.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2311be",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SailingGridworld(rock_penalty=-10, value_update_iter=30) # Set up env\n",
    "value_est, policy = get_values_policy(num_iterations=30, gamma=0.9, eps=10e-4, plot_values=False) # Run value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac0843",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88aa6c",
   "metadata": {},
   "source": [
    "<a id='T3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 3.</b> Number of Iterations Until Convergence (5 points) </h3> \n",
    "\n",
    "Set the reward for crashing into the rocks back to -2. Change the termination condition of your algorithm to make it run until convergence. **Report the number of iterations required for the value function to converge.**\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SailingGridworld(rock_penalty=-2, value_update_iter=100) # Set up env\n",
    "value_est, policy = get_values_policy(num_iterations=100, gamma=0.9, eps=10e-4, plot_values=False) # Run value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a0b24",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND REPORT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d484fdd",
   "metadata": {},
   "source": [
    "<a id='T4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 4.</b> Evaluating the Policy (10 points) </h3> \n",
    "\n",
    "Evaluate your learned policy for N = 800 episodes, and compute the discounted return of the initial state, see [1] Eq. (3.8), for each episode. The reward for crashing into rocks must be kept at -2 for this exercise. **Report the average and standard deviation of the initial state‚Äôs discounted return over the N=800 episodes.**\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SailingGridworld(rock_penalty=-2, value_update_iter=100) # Set up env\n",
    "value_est, policy = get_values_policy(num_iterations=100, gamma=0.9, eps=10e-4, plot_values=False) # Run value iteration\n",
    "mean_discounted_returns, std_discounted_returns = eval_policy(policy=policy, N=800, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7efd7f3",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND REPORT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22f560",
   "metadata": {},
   "source": [
    "<a id='Q4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.1</b> Relationship Between Discounted Return and the Value Function (10 points) </h3> \n",
    "\n",
    "What is the relationship between the discounted return and the value function? Explain briefly.\n",
    "\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef58082",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e967f1",
   "metadata": {},
   "source": [
    "<a id='Q5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.2</b> Considering Unknown Environments (15 points) </h3> \n",
    "\n",
    "Imagine a reinforcement learning problem involving a robot exploring an ***unknown*** environment. Could the ***value iteration*** **approach used in this exercise** be applied **directly** to that problem? Why/why not? Which assumptions are unrealistic, if any?\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9173695",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac5f5b",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Submitting <a id='4.'></a>\n",
    "Ensure all tasks and questions (in ```ex2.ipynb```) are answered and the relevant plots are recorded in the relevant places. Details about attaching images and figures can be found below. The relevant graphs to be included for this assignment are:\n",
    "- Task 1, image of estimated state values\n",
    "\n",
    "Ensure the correct model files are saved:\n",
    "- Task1, ensure ```value_policy.pkl``` file for the state values and the policy in Task 1 are included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad098b5f",
   "metadata": {},
   "source": [
    "## 4.1 Feedback <a id='4.1'></a>\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer feedback questionnaire in ```feedback.ipynb```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5f8d4",
   "metadata": {},
   "source": [
    "Please use the following section to record references.\n",
    "# References <a id='5.'></a>\n",
    "\n",
    "[1] Sutton R S, Barto A G. Reinforcement learning: An introduction[M]. MIT press, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118a6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
