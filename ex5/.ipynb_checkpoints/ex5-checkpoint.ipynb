{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b29c9b",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 5 - Policy Gradient </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    " \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Policy gradient with and without baseline</a>\n",
    "* <a href='#2.1'> 2.1 Training Loop</a>\n",
    "* <a href='#2.2'> 2.2 Policy with Fixed Variance </a>\n",
    "* <a href='#2.3'> 2.3 PG Objects</a>\n",
    "* <a href='#3.'> 3. Choosing the value of variance </a>\n",
    "* <a href='#3.1'> 3.1 Policy with Learned Variance </a>\n",
    "* <a href='#4.'> 4. PG and experience replay </a>\n",
    "* <a href='#5.'> 5. Real-world control problems </a>\n",
    "* <a href='#6.'> 6. Discrete action spaces </a>\n",
    "* <a href='#7.'> 7. Submitting </a>\n",
    "* <a href='#7.1'> 7.1 Feedback </a>\n",
    "* <a href='#8.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing Reinforce PG algorithm </a>\\\n",
    "<a href='#T1a'><b>Student Task 1a.</b> Basic REINFORCE without baseline (15 points) </a>\\\n",
    "<a href='#T1b'><b>Student Task 1b.</b> REINFORCE with a constant baseline b = 20 (5 points) </a>\\\n",
    "<a href='#T1c'><b>Student Task 1c.</b> REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> A good baseline (15 points) </a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Making Variance a Learnable Parameter (10 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 2.1</b> Constant vs Learnable Variance (5 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 2.2</b> Learnable Variance Initial Performance (5 points) </a> \\\n",
    "<a href='#Q4'><b>Student Question 3.</b> Considering a experience reply buffer (15 points)</a>\\\n",
    "<a href='#Q5'><b>Student Question 4.1</b> Considering an unbounded continuous action space part 1(5 points) </a>\\\n",
    "<a href='#Q6'><b>Student Question 4.2</b> Considering an unbounded continuous action space part 2 (10 points) </a>\\\n",
    "<a href='#Q7'><b>Student Question 5.</b> Considering discrete action spaces (10 points)</a>\n",
    "    \n",
    "**Total Points:** 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c043fb",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "In this exercise, we will implement the REINFORCE policy gradient algorithm for [InvertedPendulum environment](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/). The InvertedPendulum is similar to the previously used CartPole environment, except it has a continuous action space: the action can now have values in a range of [‚àí3,3]. The action value represents a numerical force applied to the cart, with magnitude representing the amount of force and sign representing the direction.\n",
    "\n",
    "## 1.2 Learning Objectives <a id='1.2'></a>\n",
    "- To understand how policy gradient works\n",
    "- To implement the REINFORCE policy gradient algorithm\n",
    "- To understand the limits and use cases of policy gradient\n",
    "\n",
    "## 1.3 Code Structure and Files <a id='1.3'></a>\n",
    "You don‚Äôt have to edit any other file other than ```ex5.ipynb``` to complete this exercise.\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                  # Config files for environments e.g. define the maximum number of steps in an episode.\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                 # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging          # Contains logged data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel            # Contains the policies learned\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo            # Contains videos for each environment\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ‚îÄInvertedPendulum-v4\n",
    "‚îÇ       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄtest      # Videos saved during testing\n",
    "‚îÇ       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄtrain     # Videos saved during training\n",
    "‚îÇ   ex5.ipynb            # Main assignment file containing tasks <---------\n",
    "‚îÇ   setup.py             # Contains setup function\n",
    "‚îÇ   utils.py             # Contains useful functions \n",
    "```\n",
    "---\n",
    "The foundational class, `PG`, stands as the base class for the Policy Gradient. Three additional classes - `PGNoBase(PG)`, `PGBase20(PG)`, and `PGGamma(PG)` - inherit from this base class. Each class corresponds to a different variant of the policy gradient:\n",
    "\n",
    "- `PGNoBase(PG)`: Implements the vanilla Policy Gradient without a baseline.\n",
    "- `PGBase20(PG)`: Utilizes a baseline of 20.\n",
    "- `PGGamma(PG)`: Employs PG gamma with discounted rewards normalized to zero mean and unit variance.\n",
    "\n",
    "# 2. Policy gradient with and without baseline <a id='2.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabd7b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing Reinforce PG algorithm (25 points) </h3> \n",
    "\n",
    "Implement the REINFORCE policy gradient algorithm to balance the InvertedPendulum. \n",
    "<br> Use constant standard deviation $\\sigma = 1$ (i.e. $\\log(\\sigma) = 0$) for the output action distribution throughout the training. Implement:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) basic REINFORCE without baseline (15 points),\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) REINFORCE with a constant baseline b = 20 (5 points),\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points),\n",
    "    \n",
    "**Complete the all the unfinished implementation in following cells (marked with ```TODOs```)**. \n",
    "    \n",
    "1. **Train Loop Function**: Finalize the `train_iteration(agent, env)` function to establish the training loop, similarly to how it was done in Exercise 1.\n",
    "2. **Policy Class with Fixed Variance**: Complete the `PolicyFixedVar(nn.Module)` class to generate a policy with a fixed variance.\n",
    "3. **Get Action Method**: Finish the `get_action(self, observation, evaluation=False)` method within the `PG` class, ensuring it can be inherited effectively.\n",
    "4. **Complete PG Classes**: Implepment the `PGNoBase(PG)`, `PGBase20(PG)`, and `PGGamma(PG)` classes, each tailored with a different baseline.\n",
    "\n",
    "**Hint:** The ```class PolicyFixedVar(nn.Module)```  contains a basic neural network structure. And we include reasonable\n",
    "hyperparameters in the ```cfg``` folder.\n",
    "\n",
    "**Hint:** Your policy should output a probability distribution over actions. A suitable and straightforward choice is to use a normal distribution (`from torch.distributions import Normal`). Compute log probabilities using the `log_prob` function of the distribution. We strongly recommend you to read the official PyTorch documentation to learn how to use the distributions and related functions.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46471b1e-91d6-48bb-bb0d-f3de4be1aa79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing system and path-related modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "# Importing PyTorch related modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Importing other third-party modules\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from IPython.display import Video  # to display videos\n",
    "\n",
    "# Importing local utility modules\n",
    "import utils as u\n",
    "from setup import setup\n",
    "\n",
    "# Setting the device for storing tensors/calculations\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e45ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Training Loop <a id='2.1'></a>\n",
    "\n",
    "In the subsequent cell, a foundational framework is provided for training a Reinforcement Learning (RL) agent employing the REINFORCE algorithm. It is constructed to efficiently monitor the agent's progress and performance throughout the training phase. Please complete the training loop, marked with `TODO`, to finalize the setup for the agent‚Äôs training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ff872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch tensor to a NumPy array.\n",
    "\n",
    "    :param tensor: A PyTorch tensor.\n",
    "    :return: The converted NumPy array.\n",
    "    \"\"\"\n",
    "    # Squeeze the tensor, move it to CPU and convert to NumPy array\n",
    "    return tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "def train_iteration(agent, env):\n",
    "    # Policy training function\n",
    "\n",
    "    # Reset the environment and observe the initial state\n",
    "    reward_sum, timesteps, done = 0, 0, False\n",
    "    obs, _ = env.reset(seed=409)\n",
    "\n",
    "    while not done:\n",
    "        # TODO: Task 1: finish the train loop, including \n",
    "        # 1. Call agent.get_action to get action and log prob of the action\n",
    "        # 2. Call env.step with the action (note: you need to convert the action into a numpy array -- use the function to_numpy for this)\n",
    "        #   (Steps 1. and 2. you can also find from the 'test' function below)\n",
    "        # 3. Store the log prob of action and reward by calling agent.record\n",
    "        # 4. Update reward_sum by adding the reward received from env.step, and increase timesteps by one\n",
    "        # 5. Use the observation you receive from env.step to call agent.get_action for the next timestep \n",
    "        #   (Hint: ensure to use the `.copy()` method to avoid unexpected behavior due to aliasing.)\n",
    "\n",
    "        ########### Your code starts here ###########\n",
    "        # Get action from the agent\n",
    "\n",
    "        # Perform the action on the environment, get new state and reward\n",
    "    \n",
    "        # Store agent's and env's outcome (so that the agent can improve its policy)\n",
    "\n",
    "        # Store total episode reward\n",
    " \n",
    "        # update observation\n",
    "\n",
    "        ########## Your codes ends here. ##########\n",
    "        if timesteps >= 1000:\n",
    "            done = True\n",
    "            \n",
    "    # Update the policy after one episode\n",
    "    info = agent.update()\n",
    "\n",
    "    # Return stats of training\n",
    "    info.update({'timesteps': timesteps,\n",
    "                'ep_reward': reward_sum,})\n",
    "    return info\n",
    "\n",
    "\n",
    "def train(agent_class, cfg_path, cfg_args={}):\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "    performance_data = []\n",
    "    \n",
    "    if cfg.save_logging: \n",
    "        u.make_dir(Path().cwd()/'results'/cfg.env_name/\"logging\")\n",
    "        L = u.Logger() # create a simple logger to record stats\n",
    "        \n",
    "    # Get state and action dimensionality\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Initialise the policy gradient agent\n",
    "    agent = agent_class(state_dim, action_dim, float(cfg.lr), float(cfg.gamma))\n",
    "    \n",
    "    for ep in range(cfg.train_episodes):\n",
    "        # collect data and update the policy\n",
    "        train_info = train_iteration(agent, env)\n",
    "        train_info.update({'episodes': ep})\n",
    "\n",
    "        if cfg.save_logging:\n",
    "            L.log(**train_info)\n",
    "        if (not cfg.silent) and (ep % 100 == 0):\n",
    "            print({\"ep\": ep, **train_info})\n",
    "\n",
    "        performance_data.append(train_info)\n",
    "        \n",
    "    u.plot_training_data(performance_data)\n",
    "    if cfg.save_model:\n",
    "        agent.save(cfg.model_path)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "\n",
    "# Function to test a trained policy\n",
    "@torch.no_grad()\n",
    "def test(agent_class, cfg_path, cfg_args={}, num_episodes=5):\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "    \n",
    "    # Get state and action dimensionality\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Initialise the policy gradient agent\n",
    "    agent = agent_class(state_dim, action_dim, float(cfg.lr), float(cfg.gamma))\n",
    "    \n",
    "    print(\"Loading model from\", cfg.model_path, \"...\")\n",
    "    agent.load(cfg.model_path)\n",
    "        \n",
    "    total_test_reward = 0\n",
    "    for ep in range(num_episodes):\n",
    "        (obs, _), done = env.reset(), False\n",
    "        test_reward = 0\n",
    "        timesteps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Similar to the training loop above -\n",
    "            # get the action, act on the environment, save total reward\n",
    "            # (evaluation=True makes the agent always return what it thinks to be\n",
    "            # the best action - there is no exploration at this point)\n",
    "            action, _ = agent.get_action(obs, evaluation=True)\n",
    "            obs, reward, done, _, info = env.step(to_numpy(action))\n",
    "            timesteps += 1\n",
    "            test_reward += reward\n",
    "            \n",
    "            if timesteps >= 1000:\n",
    "                done = True\n",
    "                \n",
    "        total_test_reward += test_reward\n",
    "\n",
    "        print(\"Test ep_reward:\", test_reward)\n",
    "\n",
    "    print(\"Average test reward:\", total_test_reward/num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac054ea1",
   "metadata": {},
   "source": [
    "## 2.2 Policy with Fixed Variance <a id='2.2'></a>\n",
    "\n",
    "This code is designed to establish a neural network policy where the variance is constant. The `PolicyFixedVar` class, which is a subclass of `nn.Module`, defines this policy. The policy uses a neural network to map states to actions, outputting a normal distribution with a mean and a standard deviation. \n",
    "\n",
    "#### Task:\n",
    "1. **Implementing actor_logstd:**\n",
    "   - Initialize `self.actor_logstd` as a torch tensor with zeros, ensuring to set the `device` parameter for appropriate storage.\n",
    "\n",
    "2. **Create and Return Distribution:**\n",
    "   - In the `forward` method, create a Normal distribution (`probs`) with the computed `action_mean` and `action_std`. This distribution represents the policy‚Äôs output given a state input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5cc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialisation function for neural network layers\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# This class defines the neural network policy\n",
    "class PolicyFixedVar(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyFixedVar, self).__init__()\n",
    "\n",
    "        # Initialise a neural network with two hidden layers (64 neurons per layer)\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "\n",
    "        # TODO: Task 1: Implement actor_logstd as a torch tensor. \n",
    "        # Hint: when creating the torch tensor, remember to set the parameter device=device \n",
    "        #       so the tensor is stored correctly on CUDA (if applicable) \n",
    "        ########## Your code starts here. ###########\n",
    "        \n",
    "        ########## Your code ends here. ##########\n",
    "\n",
    "    # Do a forward pass to map state to action\n",
    "    def forward(self, state):\n",
    "        # Get mean of a Normal distribution (the output of the neural network)\n",
    "        action_mean = self.actor_mean(state)\n",
    "\n",
    "        # Make sure action_logstd matches dimension of action_mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "\n",
    "        # Exponentiate the log std to get actual std\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        # TODO: Task 1: Create a Normal distribution with mean of 'action_mean' and standard deviation of 'action_std', and return the distribution\n",
    "        ########## Your code starts here. ###########\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de05f7e",
   "metadata": {},
   "source": [
    "## 2.3 PG Objects <a id='2.3'></a>\n",
    "\n",
    "The `PG` class is defined as the Policy Gradient base class. This class forms the foundation for implementing policy gradient algorithms, providing a structured format and placeholders for essential functions and variables used in these algorithms. The methods within this class will be used to interact with, update, and manage the policy used by the agent to make decisions in the environment.\n",
    "\n",
    "Task: Implementing the `get_action` Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e300959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class for the Policy Gradient algorithm\n",
    "class PG(object):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        # Define the neural network policy with fixed variance \n",
    "        self.policy = PolicyFixedVar(state_dim, action_dim).to(device)\n",
    "\n",
    "        # Create an optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        # Set discount factor value\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Simple buffers for action probabilities and rewards\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def record(self, action_prob, reward):\n",
    "        \"\"\" Store agent's and env's outcomes to update the agent.\"\"\"\n",
    "        self.action_probs.append(action_prob)\n",
    "        self.rewards.append(torch.tensor([reward]))\n",
    "\n",
    "    def save(self, filepath):\n",
    "        torch.save(self.policy.state_dict(), filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        self.policy.load_state_dict(torch.load(filepath))\n",
    "    \n",
    "    def get_action(self, observation, evaluation=False):\n",
    "        \"\"\"Return action and logprob of this action.\"\"\"\n",
    "\n",
    "        # Add batch dimension if necessary\n",
    "        if observation.ndim == 1: \n",
    "            observation = observation[None]\n",
    "\n",
    "        # Convert observation to a torch tensor\n",
    "        x = torch.from_numpy(observation).float().to(device)\n",
    "\n",
    "        # TODO: Task 1: Calculate action and its log_prob\n",
    "        ########## Your code starts here. ###########\n",
    "        # Hint: \n",
    "        #   1. when evaluation=True, return mean, otherwise return samples from the distribution created in self.policy.forward() function.\n",
    "        #   2. notice the shape of action and act_logprob.\n",
    "        \n",
    "        # Pass state x through the policy network (T1)\n",
    "\n",
    "        # Return mean if evaluation, else sample from the distribution\n",
    "\n",
    "        # Calculate the log probability of the action (T1)\n",
    "        \n",
    "        ########## Your code ends here. ##########\n",
    "        \n",
    "        if observation.ndim == 1: \n",
    "            action = action[0]\n",
    "\n",
    "        return action, act_logprob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61981975",
   "metadata": {},
   "source": [
    "<a id='T1a'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1a.</b> Basic REINFORCE without baseline (15 points) </h3> \n",
    "The training performance plot might look like Figure 1a. \n",
    "    <figure style=\"text-align: center\">\n",
    "    <img src=\"imgs/PGNoBase.svg\" width=\"600px\">\n",
    "    <figcaption style=\"text-align: center\"> Figure 1a: REINFORCE without baseline. </figcaption>\n",
    "    </figure>\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164da7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PGNoBase(PG):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        super().__init__(state_dim, action_dim, lr, gamma)\n",
    "    \n",
    "    def update(self):\n",
    "        # Prepare dataset used to update policy\n",
    "        action_probs = torch.stack(self.action_probs, dim=0).to(device).squeeze(-1) # shape: [batch_size,]\n",
    "        rewards = torch.stack(self.rewards, dim=0).to(device).squeeze(-1) # shape [batch_size,]\n",
    "        self.action_probs, self.rewards = [], [] # clean buffers\n",
    "        \n",
    "        # TODO: Task 1: Implement the policy gradient\n",
    "        ########## Your code starts here. ##########\n",
    "        # Hints:\n",
    "        #   1. compute discounted rewards (use the discount_rewards function offered in utils.py)\n",
    "        #   2. compute the policy gradient loss\n",
    "        #   3. update the parameters (backpropagate gradients, do the optimizer step, empty optimizer gradients afterwards so that gradients don't accumulate over updates)\n",
    "\n",
    "        # Compute discounted rewards \n",
    "\n",
    "        # Calculate the PG loss\n",
    "\n",
    "        # Backprop gradients\n",
    "\n",
    "        # Do the optimizer step \n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "\n",
    "        # if you want to log something in wandb, you can put them inside the {}, otherwise, just leave it empty.\n",
    "        return {'logstd': self.policy.actor_logstd.cpu().detach().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72fee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(PGNoBase, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml') # < 7 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab5a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(PGNoBase, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(save_video=True, testing=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a2410-d8ea-4e1f-b10a-e112816124e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd()/'results'/'InvertedPendulum-v4'/'video'/'test'/'ex5-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea079dc7",
   "metadata": {},
   "source": [
    "<a id='T1b'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1b.</b> REINFORCE with a constant baseline b = 20 (5 points) </h3> \n",
    "    The training performance plot might look like Figure 1b.\n",
    "    <figure style=\"text-align: center\" id=\"figure-1b\">\n",
    "        <img src=\"imgs/PGBase20.svg\" width=\"600px\">\n",
    "        <figcaption style=\"text-align: center\"> Figure 1b: REINFORCE with a constant baseline 20. </figcaption>\n",
    "    </figure>\n",
    "    \n",
    "üîù <a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57e167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PGBase20(PG):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        super().__init__(state_dim, action_dim, lr, gamma)\n",
    "        \n",
    "    def update(self):\n",
    "        # Prepare dataset used to update policy\n",
    "        action_probs = torch.stack(self.action_probs, dim=0).to(device).squeeze(-1) # shape: [batch_size,]\n",
    "        rewards = torch.stack(self.rewards, dim=0).to(device).squeeze(-1) # shape [batch_size,]\n",
    "        self.action_probs, self.rewards = [], [] # clean buffers\n",
    "        \n",
    "        # TODO: Task 1: Implement the policy gradient\n",
    "        ########## Your code starts here. ##########\n",
    "        # Hints:\n",
    "        #   1. compute discounted rewards (use the discount_rewards function offered in utils.py)\n",
    "        #   2. compute the policy gradient loss with the baseline\n",
    "        #   3. update the parameters (backpropagate gradients, do the optimizer step, empty optimizer gradients afterwards so that gradients don't accumulate over updates)\n",
    "\n",
    "        # Compute discounted rewards \n",
    "        \n",
    "        # Calculate the PG loss with the baseline\n",
    "\n",
    "        # Backprop gradients\n",
    "\n",
    "        # Do the optimizer step\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "\n",
    "        # if you want to log something in wandb, you can put them inside the {}, otherwise, just leave it empty.\n",
    "        return {'logstd': self.policy.actor_logstd.cpu().detach().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4d283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(PGBase20, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml') # < 7 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f26f40-49c9-48f3-8543-cae81be8185f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(PGBase20, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(save_video=True, testing=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434d9ef-523c-482a-a2e8-441ac66e15e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd() / 'results' / 'InvertedPendulum-v4' / 'video' / 'test' / 'ex5-episode-5.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c7d2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T1c'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1c.</b> REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points) </h3> \n",
    "    The training performance plot might look like Figure 1c. \n",
    "    <figure style=\"text-align: center\" id=\"figure-1c\">\n",
    "        <img src=\"imgs/PGGamma.svg\" width=\"600px\">\n",
    "        <figcaption style=\"text-align: center\"> Figure 1c: REINFORCE with discounted rewards normalized to zero mean and unit variance. </figcaption>\n",
    "    </figure>\n",
    "    \n",
    "üîù <a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bcc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PGNormalized(PG):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        super().__init__(state_dim, action_dim, lr, gamma)\n",
    "        \n",
    "    def update(self):\n",
    "        # Prepare dataset used to update policy\n",
    "        action_probs = torch.stack(self.action_probs, dim=0).to(device).squeeze(-1) # shape: [batch_size,]\n",
    "        rewards = torch.stack(self.rewards, dim=0).to(device).squeeze(-1) # shape [batch_size,]\n",
    "        self.action_probs, self.rewards = [], [] # clean buffers\n",
    "        \n",
    "        # TODO: Task 1: Implement the policy gradient\n",
    "        ########## Your code starts here. ##########\n",
    "        # Hints:\n",
    "        #   1. compute discounted rewards (use the discount_rewards function offered in utils.py)\n",
    "        #   2. compute the policy gradient loss\n",
    "        #   3. update the parameters (backpropagate gradients, do the optimizer step, empty optimizer gradients afterwards so that gradients don't accumulate over updates)\n",
    "        \n",
    "        # Compute discounted rewards and normalize them\n",
    "\n",
    "        # Calculate the PG loss\n",
    "\n",
    "        # Backprop gradients\n",
    "\n",
    "        # Do the optimizer step\n",
    "    \n",
    "        ########## Your code ends here. ##########\n",
    "\n",
    "        # if you want to log something in wandb, you can put them inside the {}, otherwise, just leave it empty.\n",
    "        return {'logstd': self.policy.actor_logstd.cpu().detach().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e236519-8afb-4ec7-ba87-3181bf822576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(PGNormalized, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml') # < 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab775f-bc7e-475f-ac41-7bbf0227bc1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(PGNormalized, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(save_video=True, testing=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bbe30-5d8f-426d-9a76-806e6586a23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd() / 'results' / 'InvertedPendulum-v4' / 'video' / 'test' / 'ex5-episode-5.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856d17c",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> A good baseline (15 points) </h3> \n",
    "\n",
    "How would you choose a good value for the baseline? Why is the training more stable when using a baseline? **Justify your answer.**\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd5c64e",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4d734",
   "metadata": {},
   "source": [
    "# 3. Choosing the value of variance <a id='3.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d6f9d",
   "metadata": {},
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Making Variance a Learnable Parameter (10 points) </h3> \n",
    "\n",
    "Implement the policy‚Äôs variance as a learnable parameter of the network and update it during training. Set the initial value $\\sigma_0^2$ to 1. REINFORCE with normalized discounted returns is used for this task.\n",
    "\n",
    "**Complete the unfinished implementation in ```PolicyLearnedVar(nn.Module)``` class (marked with ```TODOs```)**. \n",
    "    \n",
    "    \n",
    "**Hint:** To make your learned variance automatically updated by the optimizer, declare your variable inside the ```__init__``` function of the model using ```torch.nn.Parameter(some_tensor)```. \n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc8f47f",
   "metadata": {},
   "source": [
    "## 3.1 Policy with Learned Variance <a id='3.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38d70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialisation function for neural network layers\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# This class defines the neural network policy with learned variance\n",
    "class PolicyLearnedVar(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyLearnedVar, self).__init__()\n",
    "\n",
    "        # Initialise a neural network with two hidden layers (64 neurons per layer)\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "\n",
    "        # TODO: Task 2: Implement actor_logstd as a learnable parameter\n",
    "        # Use log of std to make sure std doesn't become negative during training\n",
    "        ########## Your code starts here. ###########    \n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "        \n",
    "    # Do a forward pass to map state to action\n",
    "    def forward(self, state):\n",
    "        # Get mean of a Normal distribution (the output of the neural network)\n",
    "        action_mean = self.actor_mean(state)\n",
    "\n",
    "        # Make sure action_logstd matches dimension of action_mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "\n",
    "        # Exponentiate the log std to get actual std\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        # TODO: Task 1: Create a Normal distribution with mean of 'action_mean' and standard deviation of 'action_std', and return the distribution\n",
    "        ########## Your code starts here. ###########\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe99408-3caa-43d6-aa52-652c9e1b88fe",
   "metadata": {},
   "source": [
    "Initialize the `PG` class to construct the neural network policy. Continue to employ the `get_action` function, as outlined in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d7def-5ad3-461e-a2c6-216157833223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class for the Policy Gradient algorithm\n",
    "class PGLearnedVar(PGNormalized):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        super().__init__(state_dim, action_dim, lr, gamma)\n",
    "        \n",
    "        # TODO: Task 2: Define the neural network policy (self.policy) with learned variance.\n",
    "        self.policy = None\n",
    "        ########## Your code starts here. ###########  \n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "\n",
    "        # Create an optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c5cac-1e2f-4224-a503-bc1e27871a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(PGLearnedVar, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml') # < 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d4bbd-9d79-4e23-a92a-d593a192a2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(PGLearnedVar, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(save_video=True, testing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da748d77-46d5-4597-b2f9-d9885ca24e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd() / 'results' / 'InvertedPendulum-v4' / 'video' / 'test' / 'ex5-episode-5.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3294a4",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.1</b> Constant vs Learnable Variance (5 points) </h3> \n",
    "\n",
    "What are the strong and weak sides of using either a) constant variance during training, or b) learning the variance during training? **Please explain.**       \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98b0001",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f701ff9",
   "metadata": {},
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.2</b> Learnable Variance Initial Performance (5 points) </h3> \n",
    "\n",
    "In case of a learned variance, what‚Äôs the impact of the initialization on the training performance? **Please explain.**  \n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90903af",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341103b",
   "metadata": {},
   "source": [
    "# 4. PG and experience replay <a id='4.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112bc95",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='Q4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 3.</b> Considering an experience replay buffer (15 points) </h3> \n",
    "\n",
    "Why the method implemented in this exercise could not be **directly** used with an experience replay? Which steps of the algorithm would be problematic to perform with an experience replay? How any problematic steps could be resolved? **Explain your answer**\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f76423",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b67d36",
   "metadata": {},
   "source": [
    "# 5. Real-world control problems <a id='5.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731e57a",
   "metadata": {},
   "source": [
    "<a id='Q5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.1</b> Considering an unbounded continuous action space part 1 (5 points) </h3> \n",
    "\n",
    "What could go wrong when a model with an **unbounded** continuous action space and a reward function like the one used here (+1 for survival) were to be used with a physical system?\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ed5e6",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45baf8",
   "metadata": {},
   "source": [
    "<a id='Q6'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.2</b> Considering an unbounded continuous action space part 2 (10 points) </h3> \n",
    "\n",
    "How could the problems appearing in Question 4.1 be mitigated without putting a hard limit on the actions? **Explain your answer.**\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a537818",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eacb159",
   "metadata": {},
   "source": [
    "# 6. Discrete action spaces <a id='6.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ceb81",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='Q7'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 5.</b> Considering discrete action spaces (10 points) </h3> \n",
    "\n",
    "Can policy gradient methods be used with discrete action spaces? Why/why not? Which steps of the algorithm would be problematic to perform, if any? **Explain your answer.**\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979133d-a723-4ee2-984d-4a31e93a85d6",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50da4d",
   "metadata": {},
   "source": [
    "# 7. Submitting <a id='7.'></a>\n",
    "\n",
    "Ensure all tasks and questions in `ex5.ipynb` are thoroughly answered and that all the relevant plots are displayed correctly (there is no need to save the plots in the results folder). \n",
    "\n",
    "The training performance plots are as follows:\n",
    "\n",
    "- **Task 1a, REINFORCE without baseline**: Ensure the implementation is correct and the plot is similar to the expected plot shown in the task description. Refer to [Figure 1a](#figure-1a).\n",
    "  \n",
    "- **Task 1b, REINFORCE with constant baseline b = 20**: Properly implement the algorithm with a constant baseline and plot the training performance. Refer to [Figure 1b](#figure-1b).\n",
    "  \n",
    "- **Task 1c, REINFORCE with discounted rewards normalized to zero mean and unit variance**: Implement and plot the performance ensuring the rewards are normalized correctly. Refer to [Figure 1c](#figure-1c).\n",
    "  \n",
    "- **Task 2, REINFORCE with learned variance**: Ensure that the variance is being learned and adjusted during training, and plot the training performance.\n",
    "\n",
    "No model files need to be saved for this assignment.\n",
    "\n",
    "üîù [Back to Table of Contents](#TOC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad098b5f",
   "metadata": {},
   "source": [
    "## 7.1 Feedback <a id='7.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5c6d1-c83a-4d62-9b4f-5ed33f0f6aea",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bc97b-e49b-4c18-8c41-1c45711b90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d26d7a7-acc4-48db-84a8-e366ea68a709",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb90fa-9f33-49c1-aca3-55f9a9a00d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1a = None   # Task 1a. basic REINFORCE without baseline (15 points)\n",
    "T1b = None   # Task 1b. REINFORCE with a constant baseline b = 20 (5 points)\n",
    "T1c = None   # Task 1c. REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points)\n",
    "Q1_1 = None  # Question 1.1 A good baseline (15 points)\n",
    "T2 = None    # Task 2. Making Variance a Learnable Parameter (10 points)\n",
    "Q2_1 = None  # Question 2.1 Constant vs Learnable Variance (5 points)\n",
    "Q2_2 = None  # Question 2.2 Learnable Variance Initial Performance (5 points)\n",
    "Q3 = None    # Question 3. Considering a experience reply buffer (15 points)\n",
    "Q4_1 = None  # Question 4.1 Considering an unbounded continuous action space part 1 (5 points)\n",
    "Q4_2 = None  # Question 4.2 Considering an unbounded continuous action space part 2 (10 points)\n",
    "Q5 = None    # Question 5. Considering discrete action spaces (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9387c6-00de-4414-86eb-801485a59ec7",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a53120-fcf2-4dbf-b6ad-87e5aff7dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1a = None   # Task 1a. basic REINFORCE without baseline (15 points)\n",
    "T1b = None   # Task 1b. REINFORCE with a constant baseline b = 20 (5 points)\n",
    "T1c = None   # Task 1c. REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points)\n",
    "Q1_1 = None  # Question 1.1 A good baseline (15 points)\n",
    "T2 = None    # Task 2. Making Variance a Learnable Parameter (10 points)\n",
    "Q2_1 = None  # Question 2.1 Constant vs Learnable Variance (5 points)\n",
    "Q2_2 = None  # Question 2.2 Learnable Variance Initial Performance (5 points)\n",
    "Q3 = None    # Question 3. Considering a experience reply buffer (15 points)\n",
    "Q4_1 = None  # Question 4.1 Considering an unbounded continuous action space part 1 (5 points)\n",
    "Q4_2 = None  # Question 4.2 Considering an unbounded continuous action space part 2 (10 points)\n",
    "Q5 = None    # Question 5. Considering discrete action spaces (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595057e-8666-486a-bd8c-788b4de828a7",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "And other feedback you think is worth including. Type in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53a93d-a13e-4ced-bf49-2c5b406b8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5f8d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# References <a id='8.'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
