{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba46938f",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 6 - Actor Critic part 2 </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is part of the teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Deep Deterministic Policy Gradient</a>\n",
    "* <a href='#3.'> 3. Submitting </a>\n",
    "* <a href='#3.1'> 3.1 Feedback </a>\n",
    "* <a href='#8.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implement DDPG algorithm (25 points)</a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Reasons for using off-policy data (10 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 2</b> Disadvantages of DDPG (10 points) </a>\n",
    "\n",
    "    \n",
    "**Total Points:** 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de82751-fa7f-43fd-be23-890898fb184c",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "In this assignment, we will implement the Deep Deterministic Policy Gradient (DDPG) [1] algorithm for the **HalfCheetah-v4** environment. DDPG is a reinforcement learning algorithm that uses a critic, approximated as a neural network, to estimate values. DDPG uses policy gradient methods to update a deterministic policy (actor) to follow the critic. DDPG can be applied to problems with a continuous action space.\n",
    "\n",
    "## 1.1 Task environments: <a id='1.1'></a>\n",
    "\n",
    "In this exercise, we will focus on HalfCheetah-v4 tasks:\n",
    "- HalfCheetah-v4(https://gymnasium.farama.org/environments/mujoco/half_cheetah/): The HalfCheetah is a 2-dimensional robot consisting of 9 body parts and 8 joints connecting them (including two paws). The goal is to apply torques on the joints to make the cheetah run forward (right) as fast as possible, with a positive reward allocated based on the distance moved forward and a negative reward allocated for moving backward. The torso and head of the cheetah are fixed, and the torque can only be applied on the other 6 joints over the front and back thighs (connecting to the torso), shins (connecting to the thighs) and feet (connecting to the shins).\n",
    "<figure style=\"text-align: center\">\n",
    "    <img src=\"imgs/HalfCheetah-v4.png\" width=\"300\"/>\n",
    "    <figcaption style=\"text-align: center\">  Figure 1: The HalfCheetah-v4 environment. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## 1.2 Learning Objectives: <a id='1.2'></a>\n",
    "\n",
    "- Understand the DDPG algorithm\n",
    "- Understand the difference between on-policy methods and off-policy methods\n",
    "\n",
    "## 1.3 Code Structure & Files <a id='1.3'></a>\n",
    "\n",
    "```ex6_DDPG.ipynb``` is the file needed to be modified for this part of the assignment.  \n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER PART IN ```ex6_PG_AC.ipynb```** </span>\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                            # Config files for environments\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                           # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄHalfCheetah-v4\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*HalfCheetah-v4_params.pt    # Contains trained model\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*ddpg.png               # Contains training performance plot\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄInvertedPendulum-v4\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*InvertedPendulum-v4_params.pt      # Contains trained model\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*pg_ac.png              # Contains training performance plot\n",
    "‚îÇ   ex6_DDPG.ipynb                  # 2nd assignment file containing tasks <---------This task\n",
    "‚îÇ   ex6_PG_AC.ipynb                 # 1st assignment file containing tasks <---------\n",
    "‚îÇ   train.py                        # Contains train and test functions \n",
    "‚îÇ   utils.py                        # Contains useful functions \n",
    "‚îî‚îÄ‚îÄ‚îÄbuffer.py                       # Contains buffer functions\n",
    "```\n",
    "\n",
    "## 1.4 Execution time <a id='1.4'></a>\n",
    "\n",
    "The training of DDPG might take more than 30 min depending on server load. If you have problems with the training time, you can train DDPG locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a27be2",
   "metadata": {},
   "source": [
    "# 2. Deep Deterministic Policy Gradient <a id='2.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b0c3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing DDPG (25 points) </h3> \n",
    "\n",
    "Implement the deep deterministic policy gradient (DDPG) algorithm for the HalfCheetah environment. You can refer to the code from the \"ex4_dqn.ipynb\" notebook for guidance. Additionally, if necessary, consult the paper [1] for a deeper understanding of the algorithm. You can check the training performance plot in the result folder after running the plot cell.Take Figure 2 as a reference training plot. \n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/ddpg.png\" width=\"400px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2:  Training plot of the deterministic policy gradient on the HalfCheetah environment.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Complete the unfinished implementation in the `DDPG` class (marked with ```TODOs```)**. \n",
    "    \n",
    "1. **Agent Update Function**: Finish the `_update(self, )` function within the `DDPG` class\n",
    "2. **Get Action Method**: Finish the `get_action(self, observation, evaluation=False)` function within the `DDPG` class.\n",
    "\n",
    "**Hint:** Make always sure variables have correct dimensions!\n",
    "    \n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61638d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "import copy, torch, yaml,time\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import Video\n",
    "from buffer import ReplayBuffer\n",
    "import train as t\n",
    "import utils as u\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70bace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actor-critic agent\n",
    "\n",
    "# Policy class. The policy is represented by a neural network. \n",
    "# Reminder: in DDPG the policy is deterministic.\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.max_action = max_action\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 32), nn.ReLU(),\n",
    "            nn.Linear(32, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * torch.tanh(self.actor(state))\n",
    "\n",
    "\n",
    "# Critic class. The critic is represented by a neural network.\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(state_dim+action_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1))\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        return self.value(x) # output shape [batch, 1]\n",
    "\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, state_shape, action_dim, max_action, lr, gamma, tau, batch_size, buffer_size=1e6):\n",
    "        self.name = 'ddpg'\n",
    "        state_dim = state_shape[0]\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "        self.pi = Policy(state_dim, action_dim, max_action).to(device)\n",
    "        self.pi_target = copy.deepcopy(self.pi)\n",
    "        self.pi_optim = torch.optim.Adam(self.pi.parameters(), lr=float(lr))\n",
    "\n",
    "        self.q = Critic(state_dim, action_dim).to(device)\n",
    "        self.q_target = copy.deepcopy(self.q)\n",
    "        self.q_optim = torch.optim.Adam(self.q.parameters(), lr=float(lr))\n",
    "        \n",
    "        self.buffer = ReplayBuffer(state_shape, action_dim, max_size=int(float(buffer_size)))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # used to count number of transitions in a trajectory\n",
    "        self.buffer_ptr = 0\n",
    "        self.buffer_head = 0 \n",
    "        self.random_transition = 5000 # collect 5k random data for better exploration\n",
    "    \n",
    "\n",
    "    def update(self,):\n",
    "        \"\"\" After collecting one trajectory, update the pi and q for #transition times: \"\"\"\n",
    "        info = {}\n",
    "        update_iter = self.buffer_ptr - self.buffer_head # update the network once per transition\n",
    "\n",
    "        if self.buffer_ptr > self.random_transition: # update once we have enough data\n",
    "            for _ in range(update_iter):\n",
    "                info = self._update()\n",
    "        \n",
    "        # update the buffer_head:\n",
    "        self.buffer_head = self.buffer_ptr\n",
    "        return info\n",
    "\n",
    "\n",
    "    def _update(self,):\n",
    "        # get batch data\n",
    "        batch = self.buffer.sample(self.batch_size, device=device)\n",
    "        # batch contains:\n",
    "        #    state = batch.state, shape [batch, state_dim]\n",
    "        #    action = batch.action, shape [batch, action_dim]\n",
    "        #    next_state = batch.next_state, shape [batch, state_dim]\n",
    "        #    reward = batch.reward, shape [batch, 1]\n",
    "        #    not_done = batch.not_done, shape [batch, 1]\n",
    "\n",
    "        # TODO:\n",
    "        ########## Your code starts here. ##########\n",
    "        # Hints: 1. compute the Q target with the q_target and pi_target networks\n",
    "        #        2. compute the critic loss and update the q's parameters\n",
    "        #        3. compute actor loss and update the pi's parameters\n",
    "        #        4. update the target q and pi using u.soft_update_params() (See the DQN code)\n",
    "        # compute current q\n",
    "        \n",
    "        \n",
    "        # compute target q\n",
    "        \n",
    "        \n",
    "        # compute critic loss\n",
    "\n",
    "\n",
    "        # optimize the critic\n",
    "\n",
    "\n",
    "        # compute actor loss\n",
    "\n",
    "\n",
    "        # optimize the actor\n",
    "\n",
    "\n",
    "        # update the target q and target pi using u.soft_update_params() function\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "\n",
    "\n",
    "        return {}\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_action(self, observation, evaluation=False):\n",
    "        if observation.ndim == 1: observation = observation[None] # add the batch dimension\n",
    "        x = torch.from_numpy(observation).float().to(device)\n",
    "\n",
    "        if self.buffer_ptr < self.random_transition: # collect random trajectories for better exploration.\n",
    "            action = torch.rand(self.action_dim)\n",
    "        else:\n",
    "            expl_noise = 0.1 * self.max_action # the stddev of the expl_noise if not evaluation\n",
    "            \n",
    "            # TODO:\n",
    "            ########## Your code starts here. ##########\n",
    "            # Use the policy to calculate the action to execute\n",
    "            # if evaluation equals False, add normal noise to the action, where the std of the noise is expl_noise\n",
    "            # Hint: Make sure the returned action's shape is correct.\n",
    "\n",
    "\n",
    "            ########## Your code ends here. ##########\n",
    "\n",
    "        return action, {} # just return a positional value\n",
    "\n",
    "\n",
    "    def record(self, state, action, next_state, reward, done):\n",
    "        \"\"\" Save transitions to the buffer. \"\"\"\n",
    "        self.buffer_ptr += 1\n",
    "        self.buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "    \n",
    "\n",
    "    def load(self, filepath):\n",
    "        d = torch.load(filepath)\n",
    "        self.q.load_state_dict(d['q'])\n",
    "        self.q_target.load_state_dict(d['q_target'])\n",
    "        self.pi.load_state_dict(d['pi'])\n",
    "        self.pi_target.load_state_dict(d['pi_target'])\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'q': self.q.state_dict(),\n",
    "            'q_target': self.q_target.state_dict(),\n",
    "            'pi': self.pi.state_dict(),\n",
    "            'pi_target': self.pi_target.state_dict()\n",
    "        }, filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c6f18-51e9-47e0-afcd-74f07b688c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the agent\n",
    "with open(Path().cwd()/'cfg'/'ddpg.yaml', 'r') as f:\n",
    "    cfg = u.Struct(**yaml.safe_load(f))\n",
    "\n",
    "agent = DDPG(cfg.state_shape, cfg.action_dim, cfg.max_action,\n",
    "             cfg.lr, cfg.gamma, cfg.tau, cfg.batch_size, cfg.buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbf81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the agent, training should take roughly 35min\n",
    "t.train(agent, cfg_path=Path().cwd()/'cfg'/'ddpg.yaml', cfg_args={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d3d60-4711-444e-9c3f-27a8025cc7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot training performance\n",
    "t.plot(cfg_path=Path().cwd()/'cfg'/'ddpg.yaml',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2750d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the trained policy and save a video of the test\n",
    "t.test(agent, cfg_path=Path().cwd()/'cfg'/'ddpg.yaml', cfg_args=dict(save_video=True,testing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10fd24-a4d3-41f7-9685-45a6064a7e68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd()/'results'/'HalfCheetah-v4'/'video'/'test'/'ex6-episode-9.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8934bc6",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> On-policy vs off-policy part 1 (10 points) </h3> \n",
    "\n",
    "For policy gradient methods seen in Exercise 5, we update the agent using only on-policy data, while in DDPG we can use off-policy data. Why? (Try to answer in 5 sentences)\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6663ed6",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5754f7",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> On-policy vs off-policy part 2 (10 points) </h3> \n",
    "\n",
    "A big advantage of DDPG, compared to policy gradient, is that it‚Äôs able to utilise off-policy data. What are the disadvantages of deterministic policy gradient compared to the policy gradient method implemented in ex6_PG_AC.ipynb? List two of them.\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac77d2d",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb02ea-e912-46fa-9d35-e4869b07c28b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Submitting <a id='3.'></a>\n",
    "Ensure all tasks and questions (in ```ex6_DDPG.ipynb``` and ```ex6_PG_AC.ipynb```) are answered and the necessary plots are saved in the appropriate locations. The relevant plots and files that need to be submitted for this assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `ddpg.png`: Training performance plots in terms of episode and episodic reward for DDPG\n",
    "<br>\n",
    "\n",
    "  \n",
    "- Model files:\n",
    "  - `ddpg_params.pt`: Trained model\n",
    "\n",
    "\n",
    "Ensure the model files and plots are saved in correct paths:\n",
    "- ```results/HalfCheetah-v4/ddpg.png``` Training result\n",
    "- ```results/HalfCheetah-v4/model/ddpg_params.pt``` Training Model\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex6_PG_AC.ipynb```** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a07d0e-2b83-4e02-8e9b-c457fbb35485",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58b944-9121-4827-a042-72b01ed21146",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f3e51-b088-4c7d-8ad4-4eac14e6691c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f72584-ec47-492f-8d91-beabc1bbf10d",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a68775-ff72-40c5-b29b-51de6e6df98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DDPG (20 points)\n",
    "Q1 = None   # Question 1.1 Reasons of using off-policy data (10 points)\n",
    "Q2 = None   # Question 1.2 Disadvantages of DDPG (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fb542-b18b-453a-8ce2-b3bd72b43b4a",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668114c2-b143-4971-9f3f-dc18583aef8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DDPG (20 points)\n",
    "Q1 = None   # Question 1.1 Reasons of using off-policy data (10 points)\n",
    "Q2 = None   # Question 1.2 Disadvantages of DDPG (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07196-619f-47d7-85be-6d2c2d5334d8",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994fbd2-12d1-41ef-aa8f-16e4cc266fe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5f8d4",
   "metadata": {},
   "source": [
    "# References <a id='3.'></a>\n",
    "Please use the following section to record references.\n",
    "\n",
    "[1] Timothy P. Lillicrap et al. \"Continuous control with deep reinforcement learning\" ICLR 2016 https://arxiv.org/abs/1509.02971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c7e80-ded1-4dc5-8e5e-c0fea3750392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
